# Default configuration file

[DEFAULT]

# seed
# datadir
# no_cuda
# path
# validation
# batch_size
# test_batch_size
# n
# k
# use_cnn
# c1_out_channels
# c1_k
# c1_input_shape
# dropout
# boost_strength
# weight_sparsity
# weight_sparsity_cnn
# boost_strength_factor
# k_inference_factor
# use_batch_norm
# learning_rate
# learning_rate_factor
# lr_scheduler
# lr_scheduler_params
# test_noise_every_epoch
# iterations
# repetitions
# optimizer
# momentum
# log_interval
# create_plots
# batches_in_epoch
# count_nonzeros
# min_weight
# min_dutycycle
# experiment [single, list, grid]

experiment = grid
repetitions = 1
iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
learning_rate_factor = 1.0
use_batch_norm = True
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 2000
k = 200
weight_sparsity = 0.50
weight_sparsity_cnn = 1
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = True # If False, will only test noise at end

; validation dataset ratio. Train on X%, validate on (1-X)%.
; Set to 1.0 to skip the validation step and use the whole training dataset
validation = float(50000.0/60000.0)

path = results
datadir = "data"

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = "StepLR"

; Configure lr_scheduler class constructor using kwargs style dictionary
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"

; CNN specific parameters
use_cnn = False
c1_out_channels = 10
c1_k = 6
dropout = 0.5

saveNet = False
count_nonzeros = False
min_weight = 0.0
min_dutycycle = 0.0

c1_input_shape = "1_28_28"

;[sparseNN]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.04
;learning_rate_factor = 0.5
;momentum = 0.5
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 20
;use_cnn = False
;saveNet = True

;[denseNN]
;batch_size = 16
;iterations = 10
;n = 500
;k = 500
;k_inference_factor = 1.0
;weight_sparsity = 1.0
;min_dutycycle = 0.0
;min_weight = 0.0
;boost_strength = 0.0
;boost_strength_factor = 1.0
;learning_rate = 0.04
;learning_rate_factor = 0.5
;momentum = 0.5
;dropout=0.0
;use_batch_norm = False
;use_cnn = False
;c1_k = 0
;c1_out_channels = 0
;saveNet = True

;[BestDenseOneLayer]
;n = 200
;k = 200
;boost_strength = 0.0
;learning_rate = 0.04
;learning_rate_factor = 0.9
;momentum = 0.5
;weight_sparsity = 1.0
;validation = 1.0
;log_interval = 500
;dropout = 0.0
;use_batch_norm = False

;Best one layer dense net as of 1/22/2019 from Luiz
;[BestDenseOneLayer2]
;batch_size = 16
;iterations = 10
;n = 500
;k = 500
;k_inference_factor = 1.0
;weight_sparsity = 1.0
;boost_strength = 0.0
;boost_strength_factor = 1.0
;learning_rate = 0.04
;learning_rate_factor = 0.5
;momentum = 0.5
;dropout= 0.0
;use_batch_norm = False
;use_cnn = False
;c1_k = 0
;c1_out_channels = 0


;Best one layer sparse net as of 1/21/2019
;[bestSparseNet]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.04
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 20

;Best one layer sparse net as of 1/22/2019 from Luiz
;[bestSparseNet2]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.04
;learning_rate_factor = 0.5
;momentum = 0.5
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 20
;use_cnn = False

;[bestSparseNet2Seeds]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.04
;learning_rate_factor = 0.5
;momentum = 0.5
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 20
;use_cnn = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

;[bestSparseNet2SeedsLowerLR]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.02
;learning_rate_factor = 0.5
;momentum = 0.5
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 20
;use_cnn = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

;[bestSparseNet4LowerLR]
;n = [500, 1000]
;k = [50, 100]
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.01
;learning_rate_factor = [0.5, 0.7]
;momentum = [0.5, 0.0]
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 10
;use_cnn = False

;[bestSparseNet2SeedsLowerLRNoMomentum]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.02
;learning_rate_factor = 0.5
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 20
;use_cnn = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Best one layer dense net as of 1/22/2019 from Luiz
;[BestDenseOneLayerSeeds2]
;batch_size = 16
;iterations = 10
;n = 500
;k = 500
;k_inference_factor = 1.0
;weight_sparsity = 1.0
;boost_strength = 0.0
;boost_strength_factor = 1.0
;learning_rate = 0.04
;learning_rate_factor = 0.5
;momentum = 0.5
;dropout= 0.0
;use_batch_norm = False
;use_cnn = False
;c1_k = 0
;c1_out_channels = 0
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

;[BestDenseOneLayerSeeds]
;n = 200
;k = 200
;boost_strength = 0.0
;learning_rate = 0.04
;learning_rate_factor = 0.9
;momentum = 0.5
;weight_sparsity = 1.0
;validation = 1.0
;log_interval = 500
;dropout = 0.0
;use_batch_norm = False
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
;iterations = 20

;Best one layer sparse net as of 1/21/2019
;[bestSparseNetSeeds]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.04
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 20
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

;temp
;[bestSparseNet3]
;experiment = "single"
;n = [500, 500, 750, 750, 1000, 1000, 1200, 1200]
;k = [50,  75,  75,  100, 100,  200,  120,  200]
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.02
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 25

;Count non-zeros
;[sparseNonZeros]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.04
;learning_rate_factor = 0.9
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;count_nonzeros = True
;iterations = 2
;batches_in_epoch = 10

;Tune learning rates using validation set
;[sparseLearningRatesValidation]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04, 0.02, 0.01]
;learning_rate_factor = [0.9, 0.8, 0.7]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;use_batch_norm = False

;Tune learning rates using validation set
;[sparseLearningRatesValidationBatchNorm]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04, 0.02, 0.01]
;learning_rate_factor = [0.9, 0.8, 0.7]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;use_batch_norm = True


;Tune learning rates using validation set
;[denseLearningRatesValidation]
;n = 200
;k = 200
;iterations = 20
;boost_strength = 0.0
;learning_rate = [0.04, 0.02, 0.01]
;learning_rate_factor = [0.9, 0.8, 0.7]
;momentum = 0.5
;weight_sparsity = 1.0
;log_interval = 500
;use_batch_norm = False

;Tune learning rates using validation set
;[denseLearningRatesValidationBatchNorm]
;n = 200
;k = 200
;iterations = 20
;boost_strength = 0.0
;learning_rate = [0.04, 0.02, 0.01]
;learning_rate_factor = [0.9, 0.8, 0.7]
;momentum = 0.5
;weight_sparsity = 1.0
;log_interval = 500
;use_batch_norm = True

;[bestSparseNetEntropyTest]
;n = 500
;k = 50
;boost_strength = [1.5, 0.0]
;boost_strength_factor = 0.85
;learning_rate = 0.04
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 4000
;dropout = 0.0
;validation = 1.0
;use_batch_norm = False
;iterations = 20

;[experimentQuick]
;n = 50
;k = 10
;repetitions = 1
;iterations = 2
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;weight_sparsity = 0.4
;k_inference_factor = 1.0
;use_cnn = False
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = False
;batches_in_epoch = 40
;create_plots = False
;batch_size = 40
;validation = 1.0

;[NoiseExperimentDense]
;use_batch_norm = [True, False]
;test_noise_every_epoch = True
;repetitions = 1
;iterations = 20
;batch_size = 64
;validation = 1.0
;n = 200
;k = 200
;weight_sparsity = 1.0
;k_inference_factor = 1.0
;optimizer = SGD
;learning_rate = 0.01
;lr_scheduler = StepLR
;learning_rate_factor = 1.0
;momentum = 0.5
;dropout = 0.0
;boost_strength = 0.0
;boost_strength_factor = 1.0
;batches_in_epoch = 100000
;test_batch_size = 1000
;use_cnn = False
;c1_k = 0
;c1_out_channels = 0
;count_nonzeros = False
;min_dutycycle = 0.0
;min_weight = 0.0
;no_cuda = False
;create_plots = False
;experiment = grid
;log_interval = 1000
;
;[NoiseExperimentSparse]
;use_batch_norm = [True, False]
;test_noise_every_epoch = True
;repetitions = 1
;iterations = 20
;batch_size = 4
;validation = 1.0
;n = 500
;k = 50
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;optimizer = SGD
;learning_rate = 0.04
;lr_scheduler = StepLR
;learning_rate_factor = 0.5
;momentum = 0.0
;dropout = 0.0
;boost_strength = 1.0
;boost_strength_factor = 0.9
;batches_in_epoch = 100000
;test_batch_size = 1000
;use_cnn = False
;c1_k = 0
;c1_out_channels = 0
;count_nonzeros = False
;min_dutycycle = 0.0
;min_weight = 0.0
;no_cuda = False
;create_plots = False
;experiment = grid
;log_interval = 1000


; See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
;[DropoutExperimentDense]
;n = 800
;k = 800
;boost_strength = 0.0
;learning_rate = 0.01
;momentum = 0.5
;weight_sparsity = 1.0
;dropout=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

;[DropoutExperimentDenseCNN]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 150
;iterations = 9
;boost_strength = 1.4
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;dropout=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

; parameters from [bestSparseNet]
;[DropoutExperimentSparse]
;n = 500
;k = 50
;boost_strength = 1.5
;learning_rate = 0.04
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 10
;k_inference_factor = 1.5
;boost_strength_factor = 0.9
;batch_size = 4
;dropout=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

; parameters from [bestSparseCNN]
;[DropoutExperimentSparseCNN]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 9
;boost_strength = 1.4
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;dropout=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]


;[ImpactOfN]
;use_batch_norm = [True, False]
;test_noise_every_epoch = True
;repetitions = 1
;iterations = 20
;batch_size = 4
;validation = 1.0
;n = [500,550,600,700,800,900,1000,1500]
;k = 50
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;optimizer = SGD
;learning_rate = 0.04
;lr_scheduler = StepLR
;learning_rate_factor = 0.5
;momentum = 0.0
;dropout = 0.0
;boost_strength = 1.0
;boost_strength_factor = 0.9
;batches_in_epoch = 100000
;test_batch_size = 1000
;use_cnn = False
;c1_k = 0
;c1_out_channels = 0
;count_nonzeros = False
;min_dutycycle = 0.0
;min_weight = 0.0
;no_cuda = False
;create_plots = False
;experiment = grid
;log_interval = 1000
;
;[ImpactOfK]
;use_batch_norm = [True, False]
;test_noise_every_epoch = True
;repetitions = 1
;iterations = 20
;batch_size = 4
;validation = 1.0
;n = 500
;k = [25, 50, 75, 100, 200, 300, 400, 500]
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;optimizer = SGD
;learning_rate = 0.04
;lr_scheduler = StepLR
;learning_rate_factor = 0.5
;momentum = 0.0
;dropout = 0.0
;boost_strength = 1.0
;boost_strength_factor = 0.9
;batches_in_epoch = 100000
;test_batch_size = 1000
;use_cnn = False
;c1_k = 0
;c1_out_channels = 0
;count_nonzeros = False
;min_dutycycle = 0.0
;min_weight = 0.0
;no_cuda = False
;create_plots = False
;experiment = grid
;log_interval = 1000


;[ImpactOfWeightSparsity]
;use_batch_norm = [True, False]
;test_noise_every_epoch = True
;repetitions = 1
;iterations = 20
;batch_size = 64
;validation = 1.0
;n = 200
;k = 200
;weight_sparsity = [0.2, 0.4, 0.6, 0.8, 1.0]
;k_inference_factor = 1.0
;optimizer = SGD
;learning_rate = 0.01
;lr_scheduler = StepLR
;learning_rate_factor = 1.0
;momentum = 0.5
;dropout = 0.0
;boost_strength = 0.0
;boost_strength_factor = 1.0
;batches_in_epoch = 100000
;test_batch_size = 1000
;use_cnn = False
;c1_k = 0
;c1_out_channels = 0
;count_nonzeros = False
;min_dutycycle = 0.0
;min_weight = 0.0
;no_cuda = False
;create_plots = False
;experiment = grid
;log_interval = 1000
