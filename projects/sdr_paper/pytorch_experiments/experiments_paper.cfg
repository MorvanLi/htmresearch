# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

#
# Parameters used in the paper
#

[DEFAULT]
path = results

experiment = grid
;repetitions = 1
; Uncomment to average over multiple seeds
repetitions = 10

; Uncomment to run multiple seeds as iterations
seed = 42
datadir = data
no_cuda = False
log_interval = 2000
create_plots = False
count_nonzeros = True
use_cnn = True
saveNet = False

# Common network parameters
c1_input_shape = "1_28_28"
weight_sparsity = 0.3
weight_sparsity_cnn = 1.0
use_batch_norm = False
boost_strength = 1.5
boost_strength_factor = 0.85
k_inference_factor = 1.5

# Common training regime / optimizer parameters
iterations = 15
validation = 1.0
optimizer = SGD
lr_scheduler = StepLR
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"
dropout = 0.0
batches_in_epoch = 100000
batch_size = 64
first_epoch_batch_size = %(batch_size)s
test_noise_every_epoch = True
test_batch_size = 1000
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.0

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the main networks described in the MNIST section
; of the paper.


[denseCNN1]
c1_out_channels = "30"
c1_k = "4320"
n = 1000
k = 1000
boost_strength = 0.0
momentum = 0.9
weight_sparsity = 1.0

[denseCNN2]
c1_out_channels = "30_30"
c1_k = "4320_480"
n = 1000
k = 1000
boost_strength = 0.0
momentum = 0.9
weight_sparsity = 1.0

[sparseCNN1]
c1_out_channels = 30
c1_k = 400
n = 150
k = 50
first_epoch_batch_size = 4


[sparseCNN2]
c1_out_channels = "32_64"
c1_k = "400_300"
n = 700
k = 100
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
iterations = 10


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the mixed networks described in the MNIST section
; of the paper.

# Dense CNN layers with a sparse hidden layer identical to Sparse CNN2
[denseCNN2SP3]
c1_out_channels = "30_30"
c1_k = "4320_480"
n = 700
k = 100
momentum = 0.5
first_epoch_batch_size = 4
iterations = 10

# Sparse CNN layers with a dense hidden layer like Dense CNN2
[sparseCNN2D3]
c1_out_channels = "32_64"
c1_k = "400_300"
n = 1000
k = 1000
momentum = 0.9
weight_sparsity = 1.0
iterations = 10

# Same as Sparse CNN-2 except the hidden layer has weight sparsity = 1
[sparseCNN2W1]
c1_out_channels = "32_64"
c1_k = "400_300"
n = 700
k = 100
momentum = 0.9
weight_sparsity = 1.0
iterations = 10

# Sparse CNN-2 with a hidden layer like Dense CNN-2 but with weight sparsity 0.3
[sparseCNN2DSW]
c1_out_channels = "32_64"
c1_k = "400_300"
n = 1000
k = 1000
momentum = 0.9
first_epoch_batch_size = 4
iterations = 10
