[DEFAULT]

repetitions = 1
iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
weight_decay = 0.01
learning_rate_factor = 1.0
use_batch_norm = True
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 2000
k = 200
weight_sparsity = 0.50
weight_sparsity_cnn = 1.0
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = True # If False, will only test noise at end

path = results
datadir = "data"
background_noise_dir = "_background_noise_"

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = "StepLR"

model_type = "linear"  # "cnn", "resnet9", or linear
c1_out_channels = 10
c1_k = 6
dropout = 0.5

count_nonzeros = False
c1_input_shape = "1_32_32"

;[experimentQuick]
;n = 50
;k = 10
;iterations = 4
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.04
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 2
;test_noise_every_epoch = False
;batches_in_epoch = 10
;batch_size = 16
;model_type = "linear"


;[bestSparseLinear]
;n = "1000_800"
;k = "140_140"
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor =  0.9
;learning_rate = 0.04
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 200
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 32
;model_type = "linear"
;use_batch_norm = True
;

; Gets about 96.6% test error after 15 iterations. More iters might help.
;[resnet9]
;iterations = 20
;learning_rate_factor = [0.8,0.9]
;learning_rate = [0.04, 0.05]
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = [0.0, 0.5]
;log_interval = 40
;test_noise_every_epoch = False
;batches_in_epoch = 214
;batch_size = 96
;model_type = "resnet9"

; Best resnet - multiple seeds
;[resnet9Seeds]
;iterations = 20
;learning_rate_factor = 0.8
;learning_rate = 0.04
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0
;log_interval = 40
;test_noise_every_epoch = False
;batches_in_epoch = 214
;batch_size = 96
;model_type = "resnet9"
;seed = [43, 44, 45, 46, 47, 48, 49, 50]

; Best resnet - two more seeds
[resnet9TwoMoreSeeds]
iterations = 20
learning_rate_factor = 0.8
learning_rate = 0.04
momentum = 0.9
weight_sparsity = 1.0
dropout = 0
log_interval = 40
test_noise_every_epoch = False
batches_in_epoch = 214
batch_size = 96
model_type = "resnet9"
seed = [51, 52]

;[resnetQuick]
;iterations = 3
;learning_rate_factor = 0.8
;learning_rate = 0.04
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 10
;test_noise_every_epoch = False
;batches_in_epoch = 25
;batch_size = 16
;model_type = "resnet9"

;[linear2]
;n = 1600
;k = 1600
;iterations = 15
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 40
;test_noise_every_epoch = False
;batches_in_epoch = 214
;batch_size = 96
;model_type = "linear"


;[linear3]
;n = "800_800"
;k = "800_800"
;iterations = 10
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 50
;test_noise_every_epoch = False
;batches_in_epoch = 214
;batch_size = 96
;model_type = "linear"

;[sparseLinear1]
;n = 1600
;k = 200
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"

;[sparseLinear2]
;n = "800_800"
;k = "100_100"
;k_inference_factor = 1.5
;iterations = 10
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"

;[sparseLinear3]
;n = ["800_800", "1200_800"]
;k = ["100_100", "140_140"]
;k_inference_factor = 1.5
;iterations = 10
;boost_strength = [1.0, 1.5, 2.0]
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = [0.2, 0.4]
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"

;[sparseLinear4]
;n = ["1200_1200", "1400_1400"]
;k = ["140_140"]
;k_inference_factor = 1.5
;iterations = 10
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"
;
;[sparseLinear5]
;n = ["800_800_800"]
;k = ["140_140_140", "100_100_100"]
;k_inference_factor = 1.5
;iterations = 10
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"

;[sparseLinear6]
;n = ["600_500_400", "1000_900_800"]
;k = ["150_100_100", "100_100_100"]
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = [0.7,0.9]
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = [0.4,0.3]
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"

;[sparseLinear7]
;n = "800_800"
;k = "140_140"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = [0.3, 0.4]
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"

;[sparseLinear7]
;n = "800_800"
;k = "140_140"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = [0.3, 0.4]
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"

; Use  batchnorm, and larger batches
;[sparseLinear9]
;n = "800_800"
;k = "140_140"
;k_inference_factor = 1.5
;iterations = 5
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "linear"
;use_batch_norm = True


; Use batchnorm, larger nets, higher learning rates, and larger batches
;[sparseLinear10]
;n = ["800_800", "1000_800"]
;k = "140_140"
;k_inference_factor = 1.5
;iterations = 7
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor =  0.9
;learning_rate = [0.02, 0.04]
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = [16, 32]
;model_type = "linear"
;use_batch_norm = True


;[sparseLinear11]
;n = "1000_800"
;k = "140_140"
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor =  0.9
;learning_rate = 0.04
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = [32, 64]
;model_type = "linear"
;use_batch_norm = True

