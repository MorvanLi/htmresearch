[DEFAULT]

repetitions = 1
iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
weight_decay = 0.01
learning_rate_factor = 1.0
use_batch_norm = True
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 2000
k = 200
weight_sparsity = 0.50
weight_sparsity_cnn = 1.0
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = True # If False, will only test noise at end

path = results
datadir = "data"
background_noise_dir = "_background_noise_"

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = "StepLR"

model_type = "linear"  # "cnn", "resnet9", or linear
c1_out_channels = 10
c1_k = 6
dropout = 0.5

count_nonzeros = False
c1_input_shape = "1_32_32"

;[experimentQuickCNN]
;c1_out_channels = 3
;c1_k = 10
;n = 50
;k = 10
;iterations = 1
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.04
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 2
;test_noise_every_epoch = False
;batches_in_epoch = 10
;batch_size = 16
;model_type = "cnn"
;count_nonzeros = True


; Best sparse CNN2 as of 1/21/2019
;[sparseCNN2]
;n = ["1000"]
;k = "100"
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 30
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;count_nonzeros = False

;[denseCNN2]
;n = ["1000"]
;k = ["1000"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True

;
;[sparseCNNCountNonZeros]
;n = ["1000"]
;k = "100"
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 10
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;count_nonzeros = True


;[denseCNN2CountNonZeros]
;n = ["1000"]
;k = ["1000"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 10
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True
;count_nonzeros = True

;[bestSparseCNN1]
;n = "1000_700"
;k = "200_200"
;c1_out_channels = 20
;c1_k = 500
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True

;[bestDenseCNN1]
;n = "1000_700"
;k = "200_200"
;c1_out_channels = 20
;c1_k = 500
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True


;[cnn1]
;n = "400_400"
;k = "75_75"
;c1_out_channels = 10
;c1_k = 200
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor =  0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True


;[cnn2]
;n = "500_500"
;k = "100_100"
;c1_out_channels = [10, 20]
;c1_k = [200, 300]
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor =  0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True

; Learning rate
;[cnn3]
;n = "500_500"
;k = "100_100"
;c1_out_channels = 20
;c1_k = 300
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor =  [0.9, 0.8]
;learning_rate = [0.06, 0.04, 0.02, 0.01]
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True


# Size and k
;[cnn4]
;n = ["700_700", "1000_700"]
;k = ["100_100", "200_200"]
;c1_out_channels = [20, 40]
;c1_k = [300, 500]
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True

# Higher learning rate with best one so far
;[cnn5]
;n = "700_700"
;k = "100_100"
;c1_out_channels = 20
;c1_k = 500
;k_inference_factor = 1.5
;iterations = 10
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = [0.9, 0.8]
;learning_rate = [0.04, 0.06]
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True

; More convolutional layers
;[cnn6]
;n = "400_400"
;k = "50_50"
;c1_out_channels = "32_32"
;c1_k = "600_100"
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = [0.04, 0.02, 0.01]
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True

; Two convolutional layers, test one linear layer
;[cnn7]
;n = ["800", "400"]
;k = ["100", "75", "125"]
;c1_out_channels = "64_64"
;c1_k = "600_150"
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.04
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True

; Two convolutional layers, one linear layer, different weight sparsities.
;[cnn8]
;n = ["1000"]
;k = ["100"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = [0.4, 0.3, 0.2]
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"

; Two convolutional layers, two linear layers
;[cnn9]
;n = ["1000_700", "800_800"]
;k = ["200_200", "150_150"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = [0.0, 0.1]
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True


; Two convolutional layers, two linear layers
;[cnn10]
;n = ["1000_800"]
;k = ["200_200"]
;c1_out_channels = ["64_64", "96_64"]
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = [0.0, 0.1]
;weight_sparsity = [0.2, 0.3, 0.4]
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True

; Dense network
;[cnn11]
;n = ["800"]
;k = ["800"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = [0.01, 0.02]
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True

; GPU test
;[gpuTest]
;n = "1000_700"
;k = "200_200"
;c1_out_channels = 20
;c1_k = 500
;k_inference_factor = 1.5
;iterations = 3
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True
;no_cuda = [False, True]

;[gpuTestLaptop]
;n = "1000_700"
;k = "200_200"
;c1_out_channels = 20
;c1_k = 500
;k_inference_factor = 1.5
;iterations = 3
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True

; Weight sparsity of best one
;[cnn12]
;n = ["1000", "1500"]
;k = ["100", "150"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.03
;momentum = 0.0
;weight_sparsity = [0.1, 0.2, 0.05]
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"

; Dense network w more options
;[cnn13]
;n = ["1000"]
;k = ["1000"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = [0.9, 0.8]
;learning_rate = [0.01, 0.02]
;momentum = [0.9, 0.5]
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True


; Dense network w weight sparsity
;[cnn14]
;n = ["1000"]
;k = ["1000"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
;dropout = 0.0
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True

; Sparse versions of CNN 13, with and without momentum
; 10% and 20% sparsity, smaller batch size
;[cnn15]
;n = ["1000"]
;k = ["200", "300", "100"]
;c1_out_channels = ["64_64"]
;c1_k = ["1254_160", "2500_320"]
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = [0.9, 0.0]
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;use_batch_norm = True


; Run noise test with best sparse net so far
;[cnn16]
;n = ["1000"]
;k = ["100"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.02
;momentum = 0.0
;weight_sparsity = [0.2]
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"

; Run noise test with best dense net so far
;[cnn17]
;n = ["1000"]
;k = ["1000"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = [True, False]

; Redo cnn14 with noise tests
;[cnn18]
;n = ["1000"]
;k = ["1000"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
;dropout = 0.0
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True

; Run noise test with best sparse net so far
;[cnn19]
;n = ["1000"]
;k = ["100", "200"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = [0.02, 0.01]
;momentum = 0.0
;weight_sparsity = [0.2, 0.3, 0.4]
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"


; Larger sparse
;[cnn20]
;n = ["1000", "1500", "2000"]
;k = ["100", "200", "400"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = [0.4, 0.3]
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"


; Best sparse noise results so far - try multiple seeds for error bars
;[cnn21SparseSeeds]
;n = ["1000"]
;k = "100"
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Dense network w random seeds from best CNN13
;[cnn22DenseSeeds]
;n = ["1000"]
;k = ["1000"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = [0.0, 0.5]
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Best sparse noise so far - try multiple seeds with more iterations
;[cnn23SparseSeedsMoreIters]
;n = ["1000"]
;k = "100"
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 30
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Larger sparse
;[cnn24LargerMoreSeeds]
;n = ["1500"]
;k = ["100"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Dense network w random seeds from best CNN13
;[cnn25DenseLargerSeeds]
;n = ["1500"]
;k = ["1500"]
;c1_out_channels = ["64_64"]
;c1_k = "12544_1600"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = [0.0, 0.5]
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Larger sparse with weight sparsity lower
;[cnn26LargerWeight30pct]
;n = ["1500"]
;k = ["100"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.3
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Larger sparse with weight sparsity lower
;[cnn27LargerWeight20pct]
;n = ["1500"]
;k = ["100"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.2
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Larger sparse with weight sparsity lower
; Super sparse CNN-2
;[cnn28LargerWeight10pct]
;n = ["1500"]
;k = ["100"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.1
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Larger sparse with weight sparsity lower
; See if the k value is too low from the CNN with
;[cnn28LargerWeight10pct]
;n = ["1500"]
;k = ["100"]
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.2
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]

; Best sparse noise so far - try multiple seeds with more iterations
;[cnn29SparseAllWeightSparsities]
;n = ["1000"]
;k = "100"
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"

; Dense network w different k's
;[cnn30DenseDifferentCNNK]
;n = ["1500"]
;k = ["1500"]
;c1_out_channels = ["64_64"]
;c1_k = ["12544_1600", "8400_1400", "72000_1200", "6000_1000", "4800_800", "3600_600", "2400_400", "1200_200"]
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = [0.0, 0.5]
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True


; Dense network w different k's at linear layer
;[cnn30DenseDifferentLinearK]
;n = ["1500"]
;k = ["1500", "1300", "1100", "900", "700", "500", "300", "100"]
;c1_out_channels = ["64_64"]
;c1_k = ["12544_1600"]
;k_inference_factor = 1.5
;iterations = 25
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = [0.0, 0.5]
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 64
;model_type = "cnn"
;use_batch_norm = True

;[bestSparseCNN2EntropyTest]
;n = ["1000"]
;k = "100"
;c1_out_channels = "64_64"
;c1_k = "1200_200"
;k_inference_factor = 1.5
;iterations = 30
;boost_strength = [1.5, 0.0]
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 400
;test_noise_every_epoch = True
;batches_in_epoch = 5121
;batch_size = 16
;model_type = "cnn"
