# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
path = results
seed = 42

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = True

experiment = grid
repetitions = 1

# Training
iterations = 50


[quick]
iterations = 3
batch_size = 4
batches_in_epoch = 2
test_batch_size = 4
test_batches_in_epoch = 2
network_type = dense
nblocks = "2, 4, 8, 4"
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
avg_pool_size = 2
growth_rate = 18
dense_c1_out_planes = 144


[quickSparse]
iterations = 3
batch_size = 2
batches_in_epoch = 3
test_batch_size = 3
test_batches_in_epoch = 2
nblocks = "2, 4, 8, 4"
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
conv1_sparsity = 0.1
avg_pool_size = 2
growth_rate = 18
dense_c1_out_planes = 18
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 200


# Pretty fast, and 92.25% in 40 epochs
[denseNetBest]
network_type = dense
nblocks = "2, 4, 8, 4"
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
avg_pool_size = 2
growth_rate = 72
dense_c1_out_planes = 144


# Pretty fast, and 92.25% in 40 epochs
[sparseNetBest]
network_type = dense
nblocks = "2, 4, 8, 4"
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
avg_pool_size = 2
growth_rate = 72
dense_c1_out_planes = 144


# Pretty fast, and 92.25% in 40 epochs
[denseNet32]
network_type = dense
nblocks = "2, 4, 8, 4"
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
avg_pool_size = 2
growth_rate = 72
dense_c1_out_planes = 144


# Fast and pretty accurate: 91.6% in 40 epochs
[denseNet33]
network_type = dense
nblocks = "2, 4, 8, 4"
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
avg_pool_size = 2
growth_rate = 72
dense_c1_out_planes = 72

# Fast and pretty accurate: 28secs, 91.2% accuracy
[denseNet34]
network_type = dense
nblocks = "2, 4, 8, 4"
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
avg_pool_size = 2
growth_rate = 72
dense_c1_out_planes = 36


# 92.1% acc, 0.51 noise at 0.1%
[notSoDenseNet]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
dense_c1_out_planes = 144
transition_sparsities = "0.2, 0.2, 0.4"
avg_pool_size = 2


# 91.5% acc after 40 epochs, 0.51 noise at 0.1%
[notSoDenseNet6]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
dense_c1_out_planes = 144
dense_sparsities = "0.2, 0.2, 0.2, 0.2"
transition_sparsities = "0.2, 0.2, 0.4"
;linear_sparsity = 0.2
;linear_weight_sparsity = 0.3
;linear_n = 500
avg_pool_size = 2


# 92.3% acc, 0.54 noise at 0.1%
[notSoDenseNet7]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
dense_c1_out_planes = 144
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 500
avg_pool_size = 2


# Like notSoDenseNet7, but explore linear sparsities
[notSoDenseNet8]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
dense_c1_out_planes = 144
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = [0.2, 0.1]
linear_weight_sparsity = 0.3
linear_n = [600, 800, 1000]
avg_pool_size = 2


# Like notSoDenseNet7, but explore transition sparsities
[notSoDenseNet9]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
dense_c1_out_planes = 144
transition_sparsities = ["0.2, 0.2, 0.4", "0.1, 0.1, 0.2", "0.4, 0.4, 0.4"]
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 500
avg_pool_size = 2


# Longer run with best linear sparsity from notSoDenseNet8
[notSoDenseNet10]
iterations = 80
learning_rate = 0.1
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.93
nblocks = "2, 4, 8, 4"
growth_rate = 72
dense_c1_out_planes = 144
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 800
avg_pool_size = 2


# Longer run with best linear sparsity from notSoDenseNet8 and transition from notSoDenseNet9
[notSoDenseNet11]
iterations = 80
learning_rate = 0.1
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.93
nblocks = "2, 4, 8, 4"
growth_rate = 72
dense_c1_out_planes = 144
transition_sparsities = "0.1, 0.1, 0.2"
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 800
avg_pool_size = 2


# Like notSoDenseNet7, but add conv1 sparsity
[notSoDenseNet12]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
conv1_sparsity = 0.2
dense_c1_out_planes = 144
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 500
avg_pool_size = 2


# Mostly dense, but explore conv1 sparsity only
[notSoDenseNet13]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
conv1_sparsity = [0.1, 0.2, 0.3]
dense_c1_out_planes = 144
transition_sparsities = "1.0, 1.0, 1.0"
linear_sparsity = 0.0
linear_weight_sparsity = 0.3
linear_n = [600, 800, 1000]
avg_pool_size = 2


# Shallower, with sparsity everywhere and a large linear layer
[notSoDenseNet14]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 4, 4"
growth_rate = 72
conv1_sparsity = 0.2
dense_c1_out_planes = 144
dense_sparsities = "0.2, 0.2, 0.2, 0.2"
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 1000
avg_pool_size = 2


# Exploring shallow and wide
[notSoDenseNet15]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "1, 1, 1, 1"
growth_rate = 144
conv1_sparsity = 0.2
dense_c1_out_planes = 144
dense_sparsities = "0.2, 0.2, 0.2, 0.2"
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 1000
avg_pool_size = 1

# Exploring shallow and wide
[notSoDenseNet16]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 2, 2, 2"
growth_rate = 144
conv1_sparsity = 0.2
dense_c1_out_planes = 144
dense_sparsities = "0.2, 0.2, 0.2, 0.2"
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = 0.2
linear_weight_sparsity = 0.3
linear_n = 500
avg_pool_size = 2


;Like notSoDenseNet12, but no linear layer
;Best params: conv1 sparsity 0.2, avg pool size of 2
;Learning rate: 0.000841
;Noise= 0.00, loss = 0.3847, Accuracy = 91.330%
;Noise= 0.10, loss = 2.0102, Accuracy = 60.310%
;Best noise score = 61.09% at epoch 49
[notSoDenseNet17]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
conv1_sparsity = [0.2, 0.1]
dense_c1_out_planes = 144
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = 0
linear_weight_sparsity = 0.3
linear_n = 500
avg_pool_size = [2,4]


; Like notSoDenseNet17, but try various linear layers
; Best noise score = 62.24% at epoch 49
; Noise= 0.00, loss = 0.3592, Accuracy = 91.350%
; Noise= 0.10, loss = 1.7567, Accuracy = 62.240%
; linear_sparsity = 0.2, linear_n = 400
[notSoDenseNet18]
learning_rate = 0.05
weight_decay = 0.0005
momentum = 0.9
lr_scheduler_gamma = 0.92
nblocks = "2, 4, 8, 4"
growth_rate = 72
conv1_sparsity = 0.2
dense_c1_out_planes = 144
transition_sparsities = "0.2, 0.2, 0.4"
linear_sparsity = [0.1, 0.2]
linear_weight_sparsity = 0.3
linear_n = [400, 700, 1000]
avg_pool_size = 2

